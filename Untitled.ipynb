{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "finish reading lines\n",
      "proc data 0\n",
      "proc data 1000\n",
      "proc data 2000\n",
      "proc data 3000\n",
      "proc data 4000\n",
      "proc data 5000\n",
      "proc data 6000\n",
      "proc data 7000\n",
      "proc data 8000\n",
      "proc data 9000\n",
      "proc data 10000\n",
      "mean text length: 196.016749311\n",
      "mean hyp length: 92.3061983471\n",
      "max text length: 1704\n",
      "max hyp length: 280\n",
      "min text length: 5\n",
      "min hyp length: 22\n",
      "mean text length: 35.1777961433\n",
      "mean hyp length: 16.497107438\n",
      "max text length: 303\n",
      "max hyp length: 46\n",
      "min text length: 2\n",
      "min hyp length: 4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "mode='dev'\n",
    "n_ans='same'\n",
    "n_sent=0\n",
    "span_mode='f1'  # exact or overlap or f1\n",
    "num_class=2\n",
    "predict=False\n",
    "verbose=False\n",
    "# input_data=open(r'D:\\users\\t-yicxu\\data\\squad\\\\'+mode+'-v1.1.json',encoding='utf-8')\n",
    "input_data=open(r'D:\\users\\t-yicxu\\data\\squad\\\\'+mode+'\\\\'+mode+'-stanford.json',encoding='utf-8')\n",
    "if mode=='dev':\n",
    "\tdump_data=open(r'D:\\users\\t-yicxu\\biglearn\\res_v16_dev.score.0.dump',encoding='utf-8')\n",
    "else:\n",
    "\tdump_data=open(r'D:\\users\\t-yicxu\\biglearn\\res_v16_train.score.0.dump',encoding='utf-8')\n",
    "output_file=open(r'D:\\users\\t-yicxu\\data\\squad\\entail_'+mode+'_%s_%d_%s_%dclass.tsv' %(str(n_ans),n_sent,span_mode,num_class),'w',encoding='utf-8')\n",
    "\n",
    "print('begin')\n",
    "\n",
    "texts=[]\n",
    "hyps=[]\n",
    "labels=[]\n",
    "ids=[]\n",
    "prediction={}\n",
    "\n",
    "# all_data=json.load(input_data)\n",
    "all_data={'data':[]}\n",
    "line=input_data.readline()\n",
    "# print(line)\n",
    "assert line.strip()=='SQuDA'\n",
    "for line in input_data:\n",
    "\tall_data['data'].append(json.loads(line))\n",
    "print('finish reading lines')\n",
    "\n",
    "proc_all_data=[]\n",
    "for (ii,data) in enumerate(all_data['data']):\n",
    "\tif ii % 1000==0:\n",
    "\t\tprint('proc data',ii)\n",
    "\ttry:\n",
    "\t\t# assert len(data['answer_pos'])==1\n",
    "\t\tassert len(data['answer_pos'][0])==2\n",
    "\t\tassert len(data['answer_pos'][0][0])==2\n",
    "\texcept AssertionError:\n",
    "\t\tprint(data['answer_pos'])\n",
    "\t\tinput('check')\n",
    "\tanswer_datas=data['answer_pos']\n",
    "\tans_data_collection=set()\n",
    "\tfor a_data in answer_datas:\n",
    "\t\tans_data_collection.add(((a_data[0][0],a_data[0][1]),(a_data[1][0],a_data[1][1])))\n",
    "\tnew_answers=[]\n",
    "\tfor tup in ans_data_collection:\n",
    "\t\tnew_answers.append([[tup[0][0],tup[0][1]],[tup[1][0],tup[1][1]]])\n",
    "\tif verbose:\n",
    "\t\tprint('new_answers=',new_answers)\n",
    "\tdump_line=next(dump_data)\n",
    "\tfor ans_span in new_answers:\n",
    "\n",
    "\t\tgt_sent=range(ans_span[0][0],ans_span[1][0]+1)\n",
    "\t\tgt_sent_texts=[' '.join([t for t in data['context_tokens'][k]]) for k in gt_sent]\n",
    "\t\tgt_text=' '.join(gt_sent_texts)\n",
    "\t\tques_text=' '.join(data['question_tokens'])\n",
    "\t\tn_context_token=sum([len(sent) for sent in data['context_tokens']])\n",
    "\n",
    "\n",
    "\t\t# Insert ground truth\n",
    "\t\tgt_ans_words=[]\n",
    "\t\tfor sentid in gt_sent:\n",
    "\t\t\tst_pos=ans_span[0][1] if sentid==ans_span[0][0] else 0\n",
    "\t\t\tend_pos=ans_span[1][1] if sentid==ans_span[1][0] else len(data['context_tokens'][sentid])-1\n",
    "\t\t\t# print(st_pos,end_pos)\n",
    "\t\t\tfor idx in range(st_pos,end_pos+1):\n",
    "\t\t\t\tgt_ans_words.append(data['context_tokens'][sentid][idx])\n",
    "\t\tgt_ans=' '.join(gt_ans_words)\n",
    "\t\tlabels.append(1)\n",
    "\t\ttexts.append(gt_text)\n",
    "\t\tthishyp=' '.join([ques_text,gt_ans])\n",
    "\t\thyps.append(thishyp)\n",
    "\t\tids.append(data['id']+'_gt')\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('label=%d, text=%s, hyp=%s' % (labels[-1],texts[-1],hyps[-1]))\n",
    "\t\t\tinput('check')\n",
    "\n",
    "\t\t#insert wrong texts\n",
    "\t\tavailable_sents=set(range(len(data['context_tokens'])))-set(gt_sent)\n",
    "\t\tchoose_num=min(n_sent,len(available_sents))\n",
    "\t\tif choose_num==0:\n",
    "\t\t\tcontinue\n",
    "\t\tchosen_sents=np.random.choice(list(available_sents),choose_num)\n",
    "\t\tfor sentid in chosen_sents:\n",
    "\t\t\ttexts.append(' '.join(data['context_tokens'][sentid]))\n",
    "\t\t\thyps.append(' '.join([ques_text,gt_ans]))\n",
    "\t\t\tif num_class==2:\n",
    "\t\t\t\tlabels.append(0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabels.append(3)\n",
    "\t\t\tids.append(data['id']+'_sent')\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint('label=%d, text=%s, hyp=%s' % (labels[-1],texts[-1],hyps[-1]))\n",
    "\t\t\t\tinput('check')\t\t\t\n",
    "\n",
    "\t#insert potential wrong answers\n",
    "\tspan_probs=[]\n",
    "\tspans=[]\n",
    "\t\n",
    "\ttoken_probs=dump_line.split(' ')\n",
    "\n",
    "\t# if i>=len(proc_passages):\n",
    "\t# \tbreak\n",
    "\tif len(token_probs)!=n_context_token:\n",
    "\t\tprint('question',ii)\n",
    "\t\tprint('prediction length=', len(token_probs))\n",
    "\t\tprint('tokenize length=',n_context_token)\n",
    "\t\tprint(data)\n",
    "\t\tprint(dump_line)\n",
    "\t\tinput('check')\n",
    "\t\tcontinue\n",
    "\tstartps=[]\n",
    "\tendps=[]\n",
    "\tfor (tid,tp) in enumerate(token_probs):\n",
    "\t\tpid,startp,endp=tp.split('#')\n",
    "\t\tassert tid==int(pid)\n",
    "\t\tstartps.append(float(startp))\n",
    "\t\tendps.append(float(endp))\n",
    "\t# print(startps)\n",
    "\t# print(endps)\n",
    "\t# input('check')\n",
    "\tfor id1 in range(n_context_token):\n",
    "\t\tfor id2 in range(id1,min(n_context_token,id1+15)):\n",
    "\t\t\tspan_probs.append(-startps[id1]*endps[id2])\n",
    "\t\t\tspans.append((id1,id2))\n",
    "\tspan_rank=np.argsort(span_probs)\n",
    "\tpartsum=[0]\n",
    "\tall_tokens=sum(data['context_tokens'],[])\n",
    "\tfor sid in range(len(data['context_tokens'])):\n",
    "\t\tpartsum.append(partsum[-1]+len(data['context_tokens'][sid]))\n",
    "\n",
    "\tadded_count=0\n",
    "\tif n_ans=='same':\n",
    "\t\ttarget_num=len(new_answers)\n",
    "\telse:\n",
    "\t\ttarget_num=n_ans\n",
    "\tfirst_ten_perm=np.random.permutation(10)\n",
    "\tfor tmpi in range(len(spans)):\n",
    "\t\tif tmpi<10:\n",
    "\t\t\ti=first_ten_perm[tmpi]\n",
    "\t\telse:\n",
    "\t\t\ti=tmpi\n",
    "\t\tthis_start=spans[span_rank[i]][0]\n",
    "\t\tthis_end=spans[span_rank[i]][1]\n",
    "\n",
    "\t\tthis_ans=' '.join([all_tokens[idx] for idx in range(this_start,this_end+1)])\n",
    "\t\tif i==0:\n",
    "\t\t\tprediction[data['id']]=this_ans\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('this_start=',this_start,'this_end=',this_end)\n",
    "\t\t\t# print('gt_start=',gt_span_start,'gt_end=',gt_span_end)\n",
    "\n",
    "\t\tcanuse=True\n",
    "\t\tfor a_gt_span in new_answers:\n",
    "\t\t\ta_gt_span_start=partsum[a_gt_span[0][0]]+a_gt_span[0][1]\n",
    "\t\t\ta_gt_span_end=partsum[a_gt_span[1][0]]+a_gt_span[1][1]\n",
    "\t\t\tif span_mode=='exact':\n",
    "\t\t\t\tif this_start==a_gt_span_start and this_end==a_gt_span_end:\n",
    "\t\t\t\t\tcanuse=False\n",
    "\t\t\telif span_mode=='f1':\n",
    "\t\t\t\toverlap_length=max(min(a_gt_span_end,this_end)+1-max(a_gt_span_start,this_start),0)\n",
    "\t\t\t\tif overlap_length==0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprecision=overlap_length/(this_end-this_start+1)\n",
    "\t\t\t\t\trecall=overlap_length/(a_gt_span_end - a_gt_span_start+1)\n",
    "\t\t\t\t\tf1=2*precision*recall/(precision+recall)\n",
    "\t\t\t\t\tif f1>=0.5:\n",
    "\t\t\t\t\t\tcanuse=False\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tassert span_mode=='overlap'\n",
    "\t\t\t\tif (this_start - a_gt_span_end)*(this_end - a_gt_span_start)<=0:\n",
    "\t\t\t\t\tcanuse=False\n",
    "\n",
    "\n",
    "\t\tif not canuse:\n",
    "\t\t\tcontinue\n",
    "\t\tfor start_sent in range(len(partsum)):\n",
    "\t\t\tif partsum[start_sent+1]>=this_start:\n",
    "\t\t\t\tbreak\n",
    "\t\tfor end_sent in range(len(partsum)):\n",
    "\t\t\tif partsum[end_sent]>this_end:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\ttext_sent=range(start_sent,end_sent)\n",
    "\t\tthis_text=' '.join([' '.join([t for t in data['context_tokens'][k]]) for k in text_sent])\n",
    "\t\tassert len(this_text)!=0\n",
    "\n",
    "\t\ttexts.append(this_text)\n",
    "\t\tthis_ans=' '.join([all_tokens[idx] for idx in range(this_start,this_end+1)])\n",
    "\t\tthishyp=' '.join([ques_text,this_ans])\n",
    "\t\thyps.append(thishyp)\n",
    "\t\tif num_class==2:\n",
    "\t\t\tlabels.append(0)\n",
    "\t\telse:\n",
    "\t\t\tlabels.append(2)\n",
    "\t\tids.append(data['id']+'_ans_%d' % (tmpi))\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('label=%d, text=%s, hyp=%s, id=%s' % (labels[-1],texts[-1],hyps[-1],ids[-1]))\n",
    "\t\t\tinput('check')\n",
    "\t\tadded_count+=1\n",
    "\t\tif added_count==target_num:\n",
    "\t\t\tbreak\n",
    "\n",
    "\n",
    "\n",
    "\t\t# print('inside here')\n",
    "\t\t# break\n",
    "if predict:\n",
    "\tpredict_out=open(r'D:\\users\\t-yicxu\\BiMPM_1.0\\model_data\\sample_predict_tmp.json','w',encoding='utf-8')\n",
    "\tjson.dump(prediction,predict_out,ensure_ascii=False)\n",
    "for i in range(len(hyps)):\n",
    "\tprint('%d\\t%s\\t%s\\t%s' % (labels[i], texts[i],hyps[i],ids[i]),file=output_file)\t\n",
    "text_lens=[len(a) for a in texts]\n",
    "hyp_lens=[len(a) for a in hyps]\n",
    "\n",
    "n_words_text=[]\n",
    "n_words_hyp=[]\n",
    "for i in range(len(hyps)):\n",
    "\tn_words=len(hyps[i].split(' '))\n",
    "\tn_words_hyp.append(n_words)\n",
    "\n",
    "\tn_words=len(texts[i].split(' '))\n",
    "\tn_words_text.append(n_words)\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(text_lens))\n",
    "print('mean hyp length:',np.mean(hyp_lens))\n",
    "\n",
    "print('max text length:',np.max(text_lens))\n",
    "print('max hyp length:',np.max(hyp_lens))\n",
    "\n",
    "print('min text length:',np.min(text_lens))\n",
    "print('min hyp length:',np.min(hyp_lens))\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(n_words_text))\n",
    "print('mean hyp length:',np.mean(n_words_hyp))\n",
    "\n",
    "print('max text length:',np.max(n_words_text))\n",
    "print('max hyp length:',np.max(n_words_hyp))\n",
    "print('min text length:',np.min(n_words_text))\n",
    "print('min hyp length:',np.min(n_words_hyp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_words_text=[]\n",
    "n_words_hyp=[]\n",
    "for i in range(len(hyps)):\n",
    "\tn_words=len(hyps[i].split(' '))\n",
    "\tn_words_hyp.append(n_words)\n",
    "\n",
    "\tn_words=len(texts[i].split(' '))\n",
    "\tn_words_text.append(n_words)\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(text_lens))\n",
    "print('mean hyp length:',np.mean(hyp_lens))\n",
    "\n",
    "print('max text length:',np.max(text_lens))\n",
    "print('max hyp length:',np.max(hyp_lens))\n",
    "\n",
    "print('mean text length:',np.mean(n_words_text))\n",
    "print('mean hyp length:',np.mean(n_words_hyp))\n",
    "\n",
    "print('max text length:',np.max(n_words_text))\n",
    "print('max hyp length:',np.max(n_words_hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 , 42 ( 1825 ) . In what year was Wayman v. Southard tried by the U.S. Supreme Court ? 1825 56de2c7fcffd8e1900b4b617_gt\n",
      "[ N 18 ] How many court trials did Meucci participate in ? 18 56df9dbd4a1a83140091eb99_ans_0\n",
      "( 1986 ) . when was mr. fingers ' \" can you feel it ? \" released ? 1986 57069c2652bb891400689add_gt\n",
      "Snowfall is rare . What type of weather is a rarity in Houston ? Snowfall 570a86fe6d058f1900182f4a_gt\n",
      "Compute ! Who reported that Nintendo sold 7 million NES systems ? Compute ! 57111d93a58dae1900cd6c65_gt\n",
      "U.S. ( 20 % ) ; 2 . Which country had the lowest rate of software piracy ? U.S. 5726dcc0f1498d1400e8eda9_gt\n",
      "[ Lk . 23:28 -31 ] Where are the words found in the Gospel ? Lk . 23:28 -31 572781c65951b619008f8b8b_gt\n",
      "[ n 2 ] How many previously-separate phyla did the 2007 study reclassify ? 2 572835282ca10214002da0c6_ans_0\n",
      "She died in 1945 . In which year did his wife die ? 1945 572e904bdfa6aa1500f8d149_gt\n",
      "Two bloody noses . How many bloody noses did Spielberg get in High School ? Two 572e847d03f9891900756716_gt\n",
      "Two bloody noses . How many bloody noses did Spielberg get in high school ? Two 57318ba805b4da19006bd28c_gt\n",
      "He died in 78 BC . In what year did Sulla die ? 78 BC 57301781b2c2fd1400568852_gt\n",
      "He died in 78 BC . In what year did Sulla succesfully take over the populares controlled city ? 78 BC 57301781b2c2fd1400568850_ans_3\n",
      "She fled to Xi'an . Where did Cixi go after Beijing fell to the 8 armies ? Xi'an 57314deaa5e9cc1400cdbe43_gt\n"
     ]
    }
   ],
   "source": [
    "for (i,text) in enumerate(texts):\n",
    "    if len(text)<=20:\n",
    "        print(text,hyps[i],ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_vec_path': 'D:\\\\users\\\\t-yicxu\\\\data\\\\snli_1.0\\\\word2vec_nounk_withdev.txt', 'dev_path': 'D:\\\\users\\\\t-yicxu\\\\data\\\\snli_1.0\\\\dev.tsv', 'word_level_MP_dim': -1, 'aggregation_lstm_dim': 300, 'model_dir': 'D:\\\\users\\\\t-yicxu\\\\model_data\\\\BiMPM\\\\', 'train_path': 'D:\\\\users\\\\t-yicxu\\\\data\\\\snli_1.0\\\\train.tsv', 'wo_maxpool_match': False, 'with_POS': False, 'wo_attentive_match': False, 'optimize_type': 'adam', 'context_layer_num': 2, 'with_highway': True, 'with_lex_decomposition': False, 'test_path': 'D:\\\\users\\\\t-yicxu\\\\data\\\\snli_1.0\\\\test.tsv', 'POS_dim': 20, 'char_lstm_dim': 100, 'batch_size': 60, 'max_char_per_word': 10, 'learning_rate': 0.001, 'lex_decompsition_dim': -1, 'suffix': 'snli_newconfig_3class_rerun', 'with_filter_layer': False, 'wo_full_match': False, 'lambda_l2': 0.0, 'with_aggregation_highway': True, 'wo_char': False, 'context_lstm_dim': 100, 'with_NER': False, 'with_match_highway': True, 'NER_dim': 20, 'fix_word_vec': True, 'max_sent_length': 100, 'highway_layer_num': 1, 'wo_max_attentive_match': False, 'aggregation_layer_num': 2, 'MP_dim': 10, 'wo_left_match': False, 'max_epochs': 10, 'dropout_rate': 0.1, 'char_emb_dim': 20, 'wo_right_match': False}\n",
      "MP_dim=10, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=100, base_dir='/u/zhigwang/zhigwang1/sentence_match/snli', batch_size=60, char_emb_dim=20, char_lstm_dim=100, context_layer_num=2, context_lstm_dim=100, dropout_rate=0.1, fix_word_vec=True, highway_layer_num=1, lambda_l2=0.0, learning_rate=0.001, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=10, max_sent_length=100, optimize_type='adam', suffix='snli_7', with_NER=False, with_POS=False, with_aggregation_highway=True, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_attentive_match=False, wo_char=False, wo_full_match=False, wo_left_match=False, wo_max_attentive_match=False, wo_maxpool_match=False, wo_right_match=False, word_level_MP_dim=-1\n"
     ]
    }
   ],
   "source": [
    "config1=json.load(open('config1.json'))\n",
    "config2=open('config2.json').readline()\n",
    "print(config1)\n",
    "print(config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout_rate': '0.1', 'word_level_MP_dim': '-1', 'wo_maxpool_match': 'False', 'base_dir': \"'/u/zhigwang/zhigwang1/sentence_match/snli'\", 'wo_left_match': 'False', 'with_POS': 'False', 'wo_attentive_match': 'False', 'optimize_type': \"'adam'\", 'context_layer_num': '2', 'with_highway': 'True', 'with_lex_decomposition': 'False', 'POS_dim': '20', 'aggregation_lstm_dim': '100', 'wo_right_match': 'False', 'batch_size': '60', 'max_char_per_word': '10', 'learning_rate': '0.001', 'lex_decompsition_dim': '-1', 'suffix': \"'snli_7'\", 'with_filter_layer': 'False', 'wo_full_match': 'False', 'lambda_l2': '0.0', 'with_aggregation_highway': 'True', 'wo_char': 'False', 'context_lstm_dim': '100', 'with_NER': 'False', 'with_match_highway': 'True', 'NER_dim': '20', 'fix_word_vec': 'True', 'max_sent_length': '100', 'highway_layer_num': '1', 'wo_max_attentive_match': 'False', 'aggregation_layer_num': '2', 'MP_dim': '10', 'max_epochs': '10', 'char_emb_dim': '20', 'char_lstm_dim': '100'}\n"
     ]
    }
   ],
   "source": [
    "config2_str=config2\n",
    "config2={}\n",
    "cfs=config2_str.split(',')\n",
    "\n",
    "for cf in cfs:\n",
    "    name,val=cf.strip().split('=')\n",
    "    config2[name]=val\n",
    "print(config2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dir '/u/zhigwang/zhigwang1/sentence_match/snli'\n",
      "different: optimize_type config1: adam config2: 'adam'\n",
      "different: aggregation_lstm_dim config1: 300 config2: 100\n",
      "different: suffix config1: snli_newconfig_3class_rerun config2: 'snli_7'\n"
     ]
    }
   ],
   "source": [
    "for name in config2:\n",
    "    if name not in config1:\n",
    "        print(name,config2[name])\n",
    "    else:\n",
    "        if str(config1[name])!=str(config2[name]):\n",
    "            print('different:',name, 'config1:',config1[name],'config2:',config2[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63511486  0.2872599   0.2495    ]\n",
      " [ 0.58446731  0.29140842  0.18010312]] [[ 0.90722052  0.32068919  0.93383881]\n",
      " [ 0.42959248  0.15023892  0.78009962]] [[ 0.4294227   0.49792565  0.539976  ]\n",
      " [ 0.91242794  0.78873997  0.46472936]]\n",
      "[array([[ 1.97175813,  1.10587478,  1.72331476],\n",
      "       [ 1.92648768,  1.23038733,  1.42493212]], dtype=float32)]\n",
      "[[ 1.97175808  1.10587474  1.72331481]\n",
      " [ 1.92648773  1.23038731  1.4249321 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "tf_split=tf.placeholder(tf.float32,[None,None,None])\n",
    "# tf_index=tf.placeholder(tf.int32, [None,None])\n",
    "lengths=tf.placeholder(tf.int32,[None])\n",
    "lengths2=tf.placeholder(tf.int32,[None])\n",
    "mul2=tf.placeholder(tf.float32,[None])\n",
    "mul22=tf.placeholder(tf.float32,[None,None])\n",
    "totallength=lengths+lengths2\n",
    "n_input=tf.shape(lengths)[0]\n",
    "batch_idx=tf.range(n_input)\n",
    "gat_idx=tf.stack([batch_idx,lengths-1],axis=1)\n",
    "# part_res=tf.gather_nd(tf_split,tf_index)\n",
    "part_res=tf.gather_nd(tf_split, gat_idx)\n",
    "mul_input=tf.placeholder(tf.float32,[None,None])\n",
    "mul_part_res=tf.multiply(mul_input,mul2)\n",
    "tf_c=tf_split[:,-1,:]\n",
    "\n",
    "tfadd1=tf.placeholder(tf.float32,[None,None])\n",
    "tfadd2=tf.placeholder(tf.float32,[None,None])\n",
    "tfadd3=tf.placeholder(tf.float32,[None,None])\n",
    "tf_res=tf.add_n([tfadd1,tfadd2,tfadd3])\n",
    "\n",
    "npadd1=np.random.rand(2,3)\n",
    "npadd2=np.random.rand(2,3)\n",
    "npadd3=np.random.rand(2,3)\n",
    "input_dict={tfadd1:npadd1,tfadd2:npadd2,tfadd3:npadd3}\n",
    "\n",
    "# a=np.random.rand(2,3,2)\n",
    "# # b=np.array([[[0,0],[0,2]],[[1,0],[1,2]]])\n",
    "# # b=np.array([[0,1],[1,2]])\n",
    "# # input_dict={tf_split:a,tf_index:b}\n",
    "# c=np.array([1,3])\n",
    "# d=np.array([3,4,5])\n",
    "# dp=np.array([[3],[4]])\n",
    "# ee=np.random.rand(2,3)\n",
    "# print(dp.shape)\n",
    "# # input_dict={tf_split:a, lengths:c,lengths2:d}\n",
    "# input_dict={mul_input:ee,mul2:d}\n",
    "# print(input_dict)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    res=sess.run([tf_res],feed_dict=input_dict)\n",
    "    print(npadd1,npadd2,npadd3)\n",
    "    print(res)\n",
    "    print(npadd1+npadd2+npadd3)\n",
    "#     res_c,res,res_tot=sess.run([tf_c, part_res,totallength],feed_dict=input_dict)\n",
    "\n",
    "#     print(a)\n",
    "#     print(res)\n",
    "#     print(res_c)\n",
    "#     print(res_tot)\n",
    "#     print(res_mul)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dfs-5'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dfs-{}'.format(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "a=json.loads('[1,2,3]')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9527077   0.32686635  0.18202921  0.02052652  0.01063982]\n",
      " [ 0.21421156  0.13636784  0.79516962  0.8246795   0.11275145]\n",
      " [ 0.92403561  0.11671577  0.121504    0.59632877  0.30784391]\n",
      " [ 0.4633606   0.20784781  0.68070664  0.29804337  0.28987929]\n",
      " [ 0.29365784  0.85331405  0.84003413  0.87025575  0.95485723]\n",
      " [ 0.49901331  0.57242034  0.4893404   0.9868723   0.00543614]]\n",
      "[[ 0.9527077   0.32686635  0.18202921  0.02052652  0.01063982]\n",
      " [ 0.92403561  0.11671577  0.121504    0.59632877  0.30784391]\n",
      " [ 0.29365784  0.85331405  0.84003413  0.87025575  0.95485723]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.random.rand(6,5)\n",
    "b=a[::2,:]\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
