{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proc data 0\n",
      "proc data 1000\n",
      "proc data 2000\n",
      "proc data 3000\n",
      "proc data 4000\n",
      "proc data 5000\n",
      "proc data 6000\n",
      "proc data 7000\n",
      "proc data 8000\n",
      "proc data 9000\n",
      "proc data 10000\n",
      "mean text length: 177.605559563\n",
      "mean hyp length: 83.6712164087\n",
      "max text length: 1446\n",
      "max hyp length: 280\n",
      "mean text length: nan\n",
      "mean hyp length: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflow1.0\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Anaconda2\\envs\\tensorflow1.0\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5395bb7d9b39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean hyp length:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_words_hyp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'max text length:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_words_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'max hyp length:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_words_hyp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow1.0\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   2250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2251\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[1;32m-> 2252\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   2253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\tensorflow1.0\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# small reductions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "mode='dev'\n",
    "n_ans='same'\n",
    "n_sent=1\n",
    "span_mode='overlap'  # exact or overlap\n",
    "num_class=2\n",
    "predict=False\n",
    "# input_data=open(r'D:\\users\\t-yicxu\\data\\squad\\\\'+mode+'-v1.1.json',encoding='utf-8')\n",
    "input_data=open(r'D:\\users\\t-yicxu\\data\\squad\\\\'+mode+'\\\\'+mode+'-stanford.json',encoding='utf-8')\n",
    "if mode=='dev':\n",
    "\tdump_data=open(r'D:\\users\\t-yicxu\\biglearn\\res_v16_dev.score.0.dump',encoding='utf-8')\n",
    "else:\n",
    "\tdump_data=open(r'D:\\users\\t-yicxu\\biglearn\\res_v16_train.score.0.dump',encoding='utf-8')\n",
    "output_file=open(r'D:\\users\\t-yicxu\\data\\squad\\entail_'+mode+'_%s_%d_%s_%dclass_tmp2.tsv' %(str(n_ans),n_sent,span_mode,num_class),'w',encoding='utf-8')\n",
    "\n",
    "\t\n",
    "\n",
    "texts=[]\n",
    "hyps=[]\n",
    "labels=[]\n",
    "ids=[]\n",
    "prediction={}\n",
    "\n",
    "# all_data=json.load(input_data)\n",
    "all_data={'data':[]}\n",
    "line=input_data.readline()\n",
    "# print(line)\n",
    "assert line.strip()=='SQuDA'\n",
    "for line in input_data:\n",
    "\tall_data['data'].append(json.loads(line))\n",
    "\n",
    "verbose=False\n",
    "proc_all_data=[]\n",
    "for (ii,data) in enumerate(all_data['data']):\n",
    "\tif ii % 1000==0:\n",
    "\t\tprint('proc data',ii)\n",
    "\ttry:\n",
    "\t\t# assert len(data['answer_pos'])==1\n",
    "\t\tassert len(data['answer_pos'][0])==2\n",
    "\t\tassert len(data['answer_pos'][0][0])==2\n",
    "\texcept AssertionError:\n",
    "\t\tprint(data['answer_pos'])\n",
    "\t\tinput('check')\n",
    "\tanswer_datas=data['answer_pos']\n",
    "\tans_data_collection=set()\n",
    "\tfor a_data in answer_datas:\n",
    "\t\tans_data_collection.add(((a_data[0][0],a_data[0][1]),(a_data[1][0],a_data[1][1])))\n",
    "\tnew_answers=[]\n",
    "\tfor tup in ans_data_collection:\n",
    "\t\tnew_answers.append([[tup[0][0],tup[0][1]],[tup[1][0],tup[1][1]]])\n",
    "\tif verbose:\n",
    "\t\tprint('new_answers=',new_answers)\n",
    "\tdump_line=next(dump_data)\n",
    "\tfor ans_span in new_answers:\n",
    "\n",
    "\t\tgt_sent=range(ans_span[0][0],ans_span[1][0]+1)\n",
    "\t\tgt_sent_texts=[' '.join([t for t in data['context_tokens'][k]]) for k in gt_sent]\n",
    "\t\tgt_text=' '.join(gt_sent_texts)\n",
    "\t\tques_text=' '.join(data['question_tokens'])\n",
    "\t\tn_context_token=sum([len(sent) for sent in data['context_tokens']])\n",
    "\n",
    "\n",
    "\t\t# Insert ground truth\n",
    "\t\tgt_ans_words=[]\n",
    "\t\tfor sentid in gt_sent:\n",
    "\t\t\tst_pos=ans_span[0][1] if sentid==ans_span[0][0] else 0\n",
    "\t\t\tend_pos=ans_span[1][1] if sentid==ans_span[1][0] else len(data['context_tokens'][sentid])-1\n",
    "\t\t\t# print(st_pos,end_pos)\n",
    "\t\t\tfor idx in range(st_pos,end_pos+1):\n",
    "\t\t\t\tgt_ans_words.append(data['context_tokens'][sentid][idx])\n",
    "\t\tgt_ans=' '.join(gt_ans_words)\n",
    "\t\tlabels.append(1)\n",
    "\t\ttexts.append(gt_text)\n",
    "\t\tthishyp=' '.join([ques_text,gt_ans])\n",
    "\t\thyps.append(thishyp)\n",
    "\t\tids.append(data['id']+'_gt')\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('label=%d, text=%s, hyp=%s' % (labels[-1],texts[-1],hyps[-1]))\n",
    "\t\t\tinput('check')\n",
    "\n",
    "\t\t#insert wrong texts\n",
    "\t\tavailable_sents=set(range(len(data['context_tokens'])))-set(gt_sent)\n",
    "\t\tchoose_num=min(n_sent,len(available_sents))\n",
    "\t\tif choose_num==0:\n",
    "\t\t\tcontinue\n",
    "\t\tchosen_sents=np.random.choice(list(available_sents),choose_num)\n",
    "\t\tfor sentid in chosen_sents:\n",
    "\t\t\ttexts.append(' '.join(data['context_tokens'][sentid]))\n",
    "\t\t\thyps.append(' '.join([ques_text,gt_ans]))\n",
    "\t\t\tif num_class==2:\n",
    "\t\t\t\tlabels.append(0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabels.append(3)\n",
    "\t\t\tids.append(data['id']+'_sent')\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint('label=%d, text=%s, hyp=%s' % (labels[-1],texts[-1],hyps[-1]))\n",
    "\t\t\t\tinput('check')\t\t\t\n",
    "\n",
    "\t#insert potential wrong answers\n",
    "\tspan_probs=[]\n",
    "\tspans=[]\n",
    "\t\n",
    "\ttoken_probs=dump_line.split(' ')\n",
    "\n",
    "\t# if i>=len(proc_passages):\n",
    "\t# \tbreak\n",
    "\tif len(token_probs)!=n_context_token:\n",
    "\t\tprint('question',ii)\n",
    "\t\tprint('prediction length=', len(token_probs))\n",
    "\t\tprint('tokenize length=',n_context_token)\n",
    "\t\tprint(data)\n",
    "\t\tprint(dump_line)\n",
    "\t\tinput('check')\n",
    "\t\tcontinue\n",
    "\tstartps=[]\n",
    "\tendps=[]\n",
    "\tfor (tid,tp) in enumerate(token_probs):\n",
    "\t\tpid,startp,endp=tp.split('#')\n",
    "\t\tassert tid==int(pid)\n",
    "\t\tstartps.append(float(startp))\n",
    "\t\tendps.append(float(endp))\n",
    "\t# print(startps)\n",
    "\t# print(endps)\n",
    "\t# input('check')\n",
    "\tfor id1 in range(n_context_token):\n",
    "\t\tfor id2 in range(id1,min(n_context_token,id1+15)):\n",
    "\t\t\tspan_probs.append(-startps[id1]*endps[id2])\n",
    "\t\t\tspans.append((id1,id2))\n",
    "\tspan_rank=np.argsort(span_probs)\n",
    "\tpartsum=[0]\n",
    "\tall_tokens=sum(data['context_tokens'],[])\n",
    "\tfor sid in range(len(data['context_tokens'])):\n",
    "\t\tpartsum.append(partsum[-1]+len(data['context_tokens'][sid]))\n",
    "\tgt_span_start=partsum[ans_span[0][0]]+ans_span[0][1]\n",
    "\tgt_span_end=partsum[ans_span[1][0]]+ans_span[1][1]\n",
    "\tadded_count=0\n",
    "\tif n_ans=='same':\n",
    "\t\ttarget_num=len(new_answers)\n",
    "\telse:\n",
    "\t\ttarget_num=n_ans\n",
    "\tfor i in range(len(spans)):\n",
    "\t\tthis_start=spans[span_rank[i]][0]\n",
    "\t\tthis_end=spans[span_rank[i]][1]\n",
    "\n",
    "\t\tthis_ans=' '.join([all_tokens[idx] for idx in range(this_start,this_end+1)])\n",
    "\t\tif i==0:\n",
    "\t\t\tprediction[data['id']]=this_ans\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('this_start=',this_start,'this_end=',this_end)\n",
    "\t\t\t# print('gt_start=',gt_span_start,'gt_end=',gt_span_end)\n",
    "\n",
    "\t\tcanuse=True\n",
    "\t\tfor a_gt_span in new_answers:\n",
    "\t\t\ta_gt_span_start=partsum[a_gt_span[0][0]]+a_gt_span[0][1]\n",
    "\t\t\ta_gt_span_end=partsum[a_gt_span[1][0]]+a_gt_span[1][1]\n",
    "\t\t\tif span_mode=='exact':\n",
    "\t\t\t\tif this_start==a_gt_span_start and this_end==a_gt_span_end:\n",
    "\t\t\t\t\tcanuse=False\n",
    "\t\t\telse:\n",
    "\t\t\t\tassert span_mode=='overlap'\n",
    "\t\t\t\tif (this_start - a_gt_span_end)*(this_end - a_gt_span_start)<=0:\n",
    "\t\t\t\t\tcanuse=False\n",
    "\n",
    "\n",
    "\t\tif not canuse:\n",
    "\t\t\tcontinue\n",
    "\t\ttexts.append(gt_text)\n",
    "\t\tthis_ans=' '.join([all_tokens[idx] for idx in range(this_start,this_end+1)])\n",
    "\t\tthishyp=' '.join([ques_text,this_ans])\n",
    "\t\thyps.append(thishyp)\n",
    "\t\tif num_class==2:\n",
    "\t\t\tlabels.append(0)\n",
    "\t\telse:\n",
    "\t\t\tlabels.append(2)\n",
    "\t\tids.append(data['id']+'_ans')\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('label=%d, text=%s, hyp=%s' % (labels[-1],texts[-1],hyps[-1]))\n",
    "\t\t\tinput('check')\n",
    "\t\tadded_count+=1\n",
    "\t\tif added_count==target_num:\n",
    "\t\t\tbreak\n",
    "\n",
    "\n",
    "\n",
    "\t\t# print('inside here')\n",
    "\t\t# break\n",
    "if predict:\n",
    "\tpredict_out=open(r'D:\\users\\t-yicxu\\BiMPM_1.0\\model_data\\sample_predict_tmp.json','w',encoding='utf-8')\n",
    "\tjson.dump(prediction,predict_out,ensure_ascii=False)\n",
    "for i in range(len(hyps)):\n",
    "\tprint('%d\\t%s\\t%s\\t%s' % (labels[i], hyps[i],texts[i],ids[i]),file=output_file)\t\n",
    "text_lens=[len(a) for a in texts]\n",
    "hyp_lens=[len(a) for a in hyps]\n",
    "\n",
    "n_words_text=[]\n",
    "n_words_hyp=[]\n",
    "for i in range(len(hyps)):\n",
    "\tn_words=len(hyps[i].split(' '))\n",
    "\tn_words_hyp.append(n_words)\n",
    "\n",
    "\tn_words=len(texts[i].split(' '))\n",
    "\tn_words_text.append(n_words)\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(text_lens))\n",
    "print('mean hyp length:',np.mean(hyp_lens))\n",
    "\n",
    "print('max text length:',np.max(text_lens))\n",
    "print('max hyp length:',np.max(hyp_lens))\n",
    "\n",
    "print('mean text length:',np.mean(n_words_text))\n",
    "print('mean hyp length:',np.mean(n_words_hyp))\n",
    "\n",
    "print('max text length:',np.max(n_words_text))\n",
    "print('max hyp length:',np.max(n_words_hyp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean text length: 177.605559563\n",
      "mean hyp length: 83.6712164087\n",
      "max text length: 1446\n",
      "max hyp length: 280\n",
      "mean text length: 31.8666518708\n",
      "mean hyp length: 14.9608093362\n",
      "max text length: 263\n",
      "max hyp length: 46\n"
     ]
    }
   ],
   "source": [
    "n_words_text=[]\n",
    "n_words_hyp=[]\n",
    "for i in range(len(hyps)):\n",
    "\tn_words=len(hyps[i].split(' '))\n",
    "\tn_words_hyp.append(n_words)\n",
    "\n",
    "\tn_words=len(texts[i].split(' '))\n",
    "\tn_words_text.append(n_words)\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(text_lens))\n",
    "print('mean hyp length:',np.mean(hyp_lens))\n",
    "\n",
    "print('max text length:',np.max(text_lens))\n",
    "print('max hyp length:',np.max(hyp_lens))\n",
    "\n",
    "print('mean text length:',np.mean(n_words_text))\n",
    "print('mean hyp length:',np.mean(n_words_hyp))\n",
    "\n",
    "print('max text length:',np.max(n_words_text))\n",
    "print('max hyp length:',np.max(n_words_hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
