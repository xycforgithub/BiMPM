{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "finish reading lines\n",
      "proc data 0\n",
      "proc data 1000\n",
      "proc data 2000\n",
      "proc data 3000\n",
      "proc data 4000\n",
      "proc data 5000\n",
      "proc data 6000\n",
      "proc data 7000\n",
      "proc data 8000\n",
      "proc data 9000\n",
      "proc data 10000\n",
      "mean text length: 196.016749311\n",
      "mean hyp length: 92.3061983471\n",
      "max text length: 1704\n",
      "max hyp length: 280\n",
      "min text length: 5\n",
      "min hyp length: 22\n",
      "mean text length: 35.1777961433\n",
      "mean hyp length: 16.497107438\n",
      "max text length: 303\n",
      "max hyp length: 46\n",
      "min text length: 2\n",
      "min hyp length: 4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "mode='dev'\n",
    "n_ans='same'\n",
    "n_sent=0\n",
    "span_mode='f1'  # exact or overlap or f1\n",
    "num_class=2\n",
    "predict=False\n",
    "verbose=False\n",
    "# input_data=open(r'D:\\users\\t-yicxu\\data\\squad\\\\'+mode+'-v1.1.json',encoding='utf-8')\n",
    "input_data=open(r'D:\\users\\t-yicxu\\data\\squad\\\\'+mode+'\\\\'+mode+'-stanford.json',encoding='utf-8')\n",
    "if mode=='dev':\n",
    "\tdump_data=open(r'D:\\users\\t-yicxu\\biglearn\\res_v16_dev.score.0.dump',encoding='utf-8')\n",
    "else:\n",
    "\tdump_data=open(r'D:\\users\\t-yicxu\\biglearn\\res_v16_train.score.0.dump',encoding='utf-8')\n",
    "output_file=open(r'D:\\users\\t-yicxu\\data\\squad\\entail_'+mode+'_%s_%d_%s_%dclass.tsv' %(str(n_ans),n_sent,span_mode,num_class),'w',encoding='utf-8')\n",
    "\n",
    "print('begin')\n",
    "\n",
    "texts=[]\n",
    "hyps=[]\n",
    "labels=[]\n",
    "ids=[]\n",
    "prediction={}\n",
    "\n",
    "# all_data=json.load(input_data)\n",
    "all_data={'data':[]}\n",
    "line=input_data.readline()\n",
    "# print(line)\n",
    "assert line.strip()=='SQuDA'\n",
    "for line in input_data:\n",
    "\tall_data['data'].append(json.loads(line))\n",
    "print('finish reading lines')\n",
    "\n",
    "proc_all_data=[]\n",
    "for (ii,data) in enumerate(all_data['data']):\n",
    "\tif ii % 1000==0:\n",
    "\t\tprint('proc data',ii)\n",
    "\ttry:\n",
    "\t\t# assert len(data['answer_pos'])==1\n",
    "\t\tassert len(data['answer_pos'][0])==2\n",
    "\t\tassert len(data['answer_pos'][0][0])==2\n",
    "\texcept AssertionError:\n",
    "\t\tprint(data['answer_pos'])\n",
    "\t\tinput('check')\n",
    "\tanswer_datas=data['answer_pos']\n",
    "\tans_data_collection=set()\n",
    "\tfor a_data in answer_datas:\n",
    "\t\tans_data_collection.add(((a_data[0][0],a_data[0][1]),(a_data[1][0],a_data[1][1])))\n",
    "\tnew_answers=[]\n",
    "\tfor tup in ans_data_collection:\n",
    "\t\tnew_answers.append([[tup[0][0],tup[0][1]],[tup[1][0],tup[1][1]]])\n",
    "\tif verbose:\n",
    "\t\tprint('new_answers=',new_answers)\n",
    "\tdump_line=next(dump_data)\n",
    "\tfor ans_span in new_answers:\n",
    "\n",
    "\t\tgt_sent=range(ans_span[0][0],ans_span[1][0]+1)\n",
    "\t\tgt_sent_texts=[' '.join([t for t in data['context_tokens'][k]]) for k in gt_sent]\n",
    "\t\tgt_text=' '.join(gt_sent_texts)\n",
    "\t\tques_text=' '.join(data['question_tokens'])\n",
    "\t\tn_context_token=sum([len(sent) for sent in data['context_tokens']])\n",
    "\n",
    "\n",
    "\t\t# Insert ground truth\n",
    "\t\tgt_ans_words=[]\n",
    "\t\tfor sentid in gt_sent:\n",
    "\t\t\tst_pos=ans_span[0][1] if sentid==ans_span[0][0] else 0\n",
    "\t\t\tend_pos=ans_span[1][1] if sentid==ans_span[1][0] else len(data['context_tokens'][sentid])-1\n",
    "\t\t\t# print(st_pos,end_pos)\n",
    "\t\t\tfor idx in range(st_pos,end_pos+1):\n",
    "\t\t\t\tgt_ans_words.append(data['context_tokens'][sentid][idx])\n",
    "\t\tgt_ans=' '.join(gt_ans_words)\n",
    "\t\tlabels.append(1)\n",
    "\t\ttexts.append(gt_text)\n",
    "\t\tthishyp=' '.join([ques_text,gt_ans])\n",
    "\t\thyps.append(thishyp)\n",
    "\t\tids.append(data['id']+'_gt')\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('label=%d, text=%s, hyp=%s' % (labels[-1],texts[-1],hyps[-1]))\n",
    "\t\t\tinput('check')\n",
    "\n",
    "\t\t#insert wrong texts\n",
    "\t\tavailable_sents=set(range(len(data['context_tokens'])))-set(gt_sent)\n",
    "\t\tchoose_num=min(n_sent,len(available_sents))\n",
    "\t\tif choose_num==0:\n",
    "\t\t\tcontinue\n",
    "\t\tchosen_sents=np.random.choice(list(available_sents),choose_num)\n",
    "\t\tfor sentid in chosen_sents:\n",
    "\t\t\ttexts.append(' '.join(data['context_tokens'][sentid]))\n",
    "\t\t\thyps.append(' '.join([ques_text,gt_ans]))\n",
    "\t\t\tif num_class==2:\n",
    "\t\t\t\tlabels.append(0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabels.append(3)\n",
    "\t\t\tids.append(data['id']+'_sent')\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint('label=%d, text=%s, hyp=%s' % (labels[-1],texts[-1],hyps[-1]))\n",
    "\t\t\t\tinput('check')\t\t\t\n",
    "\n",
    "\t#insert potential wrong answers\n",
    "\tspan_probs=[]\n",
    "\tspans=[]\n",
    "\t\n",
    "\ttoken_probs=dump_line.split(' ')\n",
    "\n",
    "\t# if i>=len(proc_passages):\n",
    "\t# \tbreak\n",
    "\tif len(token_probs)!=n_context_token:\n",
    "\t\tprint('question',ii)\n",
    "\t\tprint('prediction length=', len(token_probs))\n",
    "\t\tprint('tokenize length=',n_context_token)\n",
    "\t\tprint(data)\n",
    "\t\tprint(dump_line)\n",
    "\t\tinput('check')\n",
    "\t\tcontinue\n",
    "\tstartps=[]\n",
    "\tendps=[]\n",
    "\tfor (tid,tp) in enumerate(token_probs):\n",
    "\t\tpid,startp,endp=tp.split('#')\n",
    "\t\tassert tid==int(pid)\n",
    "\t\tstartps.append(float(startp))\n",
    "\t\tendps.append(float(endp))\n",
    "\t# print(startps)\n",
    "\t# print(endps)\n",
    "\t# input('check')\n",
    "\tfor id1 in range(n_context_token):\n",
    "\t\tfor id2 in range(id1,min(n_context_token,id1+15)):\n",
    "\t\t\tspan_probs.append(-startps[id1]*endps[id2])\n",
    "\t\t\tspans.append((id1,id2))\n",
    "\tspan_rank=np.argsort(span_probs)\n",
    "\tpartsum=[0]\n",
    "\tall_tokens=sum(data['context_tokens'],[])\n",
    "\tfor sid in range(len(data['context_tokens'])):\n",
    "\t\tpartsum.append(partsum[-1]+len(data['context_tokens'][sid]))\n",
    "\n",
    "\tadded_count=0\n",
    "\tif n_ans=='same':\n",
    "\t\ttarget_num=len(new_answers)\n",
    "\telse:\n",
    "\t\ttarget_num=n_ans\n",
    "\tfirst_ten_perm=np.random.permutation(10)\n",
    "\tfor tmpi in range(len(spans)):\n",
    "\t\tif tmpi<10:\n",
    "\t\t\ti=first_ten_perm[tmpi]\n",
    "\t\telse:\n",
    "\t\t\ti=tmpi\n",
    "\t\tthis_start=spans[span_rank[i]][0]\n",
    "\t\tthis_end=spans[span_rank[i]][1]\n",
    "\n",
    "\t\tthis_ans=' '.join([all_tokens[idx] for idx in range(this_start,this_end+1)])\n",
    "\t\tif i==0:\n",
    "\t\t\tprediction[data['id']]=this_ans\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('this_start=',this_start,'this_end=',this_end)\n",
    "\t\t\t# print('gt_start=',gt_span_start,'gt_end=',gt_span_end)\n",
    "\n",
    "\t\tcanuse=True\n",
    "\t\tfor a_gt_span in new_answers:\n",
    "\t\t\ta_gt_span_start=partsum[a_gt_span[0][0]]+a_gt_span[0][1]\n",
    "\t\t\ta_gt_span_end=partsum[a_gt_span[1][0]]+a_gt_span[1][1]\n",
    "\t\t\tif span_mode=='exact':\n",
    "\t\t\t\tif this_start==a_gt_span_start and this_end==a_gt_span_end:\n",
    "\t\t\t\t\tcanuse=False\n",
    "\t\t\telif span_mode=='f1':\n",
    "\t\t\t\toverlap_length=max(min(a_gt_span_end,this_end)+1-max(a_gt_span_start,this_start),0)\n",
    "\t\t\t\tif overlap_length==0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprecision=overlap_length/(this_end-this_start+1)\n",
    "\t\t\t\t\trecall=overlap_length/(a_gt_span_end - a_gt_span_start+1)\n",
    "\t\t\t\t\tf1=2*precision*recall/(precision+recall)\n",
    "\t\t\t\t\tif f1>=0.5:\n",
    "\t\t\t\t\t\tcanuse=False\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tassert span_mode=='overlap'\n",
    "\t\t\t\tif (this_start - a_gt_span_end)*(this_end - a_gt_span_start)<=0:\n",
    "\t\t\t\t\tcanuse=False\n",
    "\n",
    "\n",
    "\t\tif not canuse:\n",
    "\t\t\tcontinue\n",
    "\t\tfor start_sent in range(len(partsum)):\n",
    "\t\t\tif partsum[start_sent+1]>=this_start:\n",
    "\t\t\t\tbreak\n",
    "\t\tfor end_sent in range(len(partsum)):\n",
    "\t\t\tif partsum[end_sent]>this_end:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\ttext_sent=range(start_sent,end_sent)\n",
    "\t\tthis_text=' '.join([' '.join([t for t in data['context_tokens'][k]]) for k in text_sent])\n",
    "\t\tassert len(this_text)!=0\n",
    "\n",
    "\t\ttexts.append(this_text)\n",
    "\t\tthis_ans=' '.join([all_tokens[idx] for idx in range(this_start,this_end+1)])\n",
    "\t\tthishyp=' '.join([ques_text,this_ans])\n",
    "\t\thyps.append(thishyp)\n",
    "\t\tif num_class==2:\n",
    "\t\t\tlabels.append(0)\n",
    "\t\telse:\n",
    "\t\t\tlabels.append(2)\n",
    "\t\tids.append(data['id']+'_ans_%d' % (tmpi))\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('label=%d, text=%s, hyp=%s, id=%s' % (labels[-1],texts[-1],hyps[-1],ids[-1]))\n",
    "\t\t\tinput('check')\n",
    "\t\tadded_count+=1\n",
    "\t\tif added_count==target_num:\n",
    "\t\t\tbreak\n",
    "\n",
    "\n",
    "\n",
    "\t\t# print('inside here')\n",
    "\t\t# break\n",
    "if predict:\n",
    "\tpredict_out=open(r'D:\\users\\t-yicxu\\BiMPM_1.0\\model_data\\sample_predict_tmp.json','w',encoding='utf-8')\n",
    "\tjson.dump(prediction,predict_out,ensure_ascii=False)\n",
    "for i in range(len(hyps)):\n",
    "\tprint('%d\\t%s\\t%s\\t%s' % (labels[i], texts[i],hyps[i],ids[i]),file=output_file)\t\n",
    "text_lens=[len(a) for a in texts]\n",
    "hyp_lens=[len(a) for a in hyps]\n",
    "\n",
    "n_words_text=[]\n",
    "n_words_hyp=[]\n",
    "for i in range(len(hyps)):\n",
    "\tn_words=len(hyps[i].split(' '))\n",
    "\tn_words_hyp.append(n_words)\n",
    "\n",
    "\tn_words=len(texts[i].split(' '))\n",
    "\tn_words_text.append(n_words)\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(text_lens))\n",
    "print('mean hyp length:',np.mean(hyp_lens))\n",
    "\n",
    "print('max text length:',np.max(text_lens))\n",
    "print('max hyp length:',np.max(hyp_lens))\n",
    "\n",
    "print('min text length:',np.min(text_lens))\n",
    "print('min hyp length:',np.min(hyp_lens))\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(n_words_text))\n",
    "print('mean hyp length:',np.mean(n_words_hyp))\n",
    "\n",
    "print('max text length:',np.max(n_words_text))\n",
    "print('max hyp length:',np.max(n_words_hyp))\n",
    "print('min text length:',np.min(n_words_text))\n",
    "print('min hyp length:',np.min(n_words_hyp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_words_text=[]\n",
    "n_words_hyp=[]\n",
    "for i in range(len(hyps)):\n",
    "\tn_words=len(hyps[i].split(' '))\n",
    "\tn_words_hyp.append(n_words)\n",
    "\n",
    "\tn_words=len(texts[i].split(' '))\n",
    "\tn_words_text.append(n_words)\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(text_lens))\n",
    "print('mean hyp length:',np.mean(hyp_lens))\n",
    "\n",
    "print('max text length:',np.max(text_lens))\n",
    "print('max hyp length:',np.max(hyp_lens))\n",
    "\n",
    "print('mean text length:',np.mean(n_words_text))\n",
    "print('mean hyp length:',np.mean(n_words_hyp))\n",
    "\n",
    "print('max text length:',np.max(n_words_text))\n",
    "print('max hyp length:',np.max(n_words_hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 , 42 ( 1825 ) . In what year was Wayman v. Southard tried by the U.S. Supreme Court ? 1825 56de2c7fcffd8e1900b4b617_gt\n",
      "[ N 18 ] How many court trials did Meucci participate in ? 18 56df9dbd4a1a83140091eb99_ans_0\n",
      "( 1986 ) . when was mr. fingers ' \" can you feel it ? \" released ? 1986 57069c2652bb891400689add_gt\n",
      "Snowfall is rare . What type of weather is a rarity in Houston ? Snowfall 570a86fe6d058f1900182f4a_gt\n",
      "Compute ! Who reported that Nintendo sold 7 million NES systems ? Compute ! 57111d93a58dae1900cd6c65_gt\n",
      "U.S. ( 20 % ) ; 2 . Which country had the lowest rate of software piracy ? U.S. 5726dcc0f1498d1400e8eda9_gt\n",
      "[ Lk . 23:28 -31 ] Where are the words found in the Gospel ? Lk . 23:28 -31 572781c65951b619008f8b8b_gt\n",
      "[ n 2 ] How many previously-separate phyla did the 2007 study reclassify ? 2 572835282ca10214002da0c6_ans_0\n",
      "She died in 1945 . In which year did his wife die ? 1945 572e904bdfa6aa1500f8d149_gt\n",
      "Two bloody noses . How many bloody noses did Spielberg get in High School ? Two 572e847d03f9891900756716_gt\n",
      "Two bloody noses . How many bloody noses did Spielberg get in high school ? Two 57318ba805b4da19006bd28c_gt\n",
      "He died in 78 BC . In what year did Sulla die ? 78 BC 57301781b2c2fd1400568852_gt\n",
      "He died in 78 BC . In what year did Sulla succesfully take over the populares controlled city ? 78 BC 57301781b2c2fd1400568850_ans_3\n",
      "She fled to Xi'an . Where did Cixi go after Beijing fell to the 8 armies ? Xi'an 57314deaa5e9cc1400cdbe43_gt\n"
     ]
    }
   ],
   "source": [
    "for (i,text) in enumerate(texts):\n",
    "    if len(text)<=20:\n",
    "        print(text,hyps[i],ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_vec_path': 'D:\\\\users\\\\t-yicxu\\\\data\\\\snli_1.0\\\\word2vec_nounk_withdev.txt', 'dev_path': 'D:\\\\users\\\\t-yicxu\\\\data\\\\snli_1.0\\\\dev.tsv', 'word_level_MP_dim': -1, 'aggregation_lstm_dim': 300, 'model_dir': 'D:\\\\users\\\\t-yicxu\\\\model_data\\\\BiMPM\\\\', 'train_path': 'D:\\\\users\\\\t-yicxu\\\\data\\\\snli_1.0\\\\train.tsv', 'wo_maxpool_match': False, 'with_POS': False, 'wo_attentive_match': False, 'optimize_type': 'adam', 'context_layer_num': 2, 'with_highway': True, 'with_lex_decomposition': False, 'test_path': 'D:\\\\users\\\\t-yicxu\\\\data\\\\snli_1.0\\\\test.tsv', 'POS_dim': 20, 'char_lstm_dim': 100, 'batch_size': 60, 'max_char_per_word': 10, 'learning_rate': 0.001, 'lex_decompsition_dim': -1, 'suffix': 'snli_newconfig_3class_rerun', 'with_filter_layer': False, 'wo_full_match': False, 'lambda_l2': 0.0, 'with_aggregation_highway': True, 'wo_char': False, 'context_lstm_dim': 100, 'with_NER': False, 'with_match_highway': True, 'NER_dim': 20, 'fix_word_vec': True, 'max_sent_length': 100, 'highway_layer_num': 1, 'wo_max_attentive_match': False, 'aggregation_layer_num': 2, 'MP_dim': 10, 'wo_left_match': False, 'max_epochs': 10, 'dropout_rate': 0.1, 'char_emb_dim': 20, 'wo_right_match': False}\n",
      "MP_dim=10, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=100, base_dir='/u/zhigwang/zhigwang1/sentence_match/snli', batch_size=60, char_emb_dim=20, char_lstm_dim=100, context_layer_num=2, context_lstm_dim=100, dropout_rate=0.1, fix_word_vec=True, highway_layer_num=1, lambda_l2=0.0, learning_rate=0.001, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=10, max_sent_length=100, optimize_type='adam', suffix='snli_7', with_NER=False, with_POS=False, with_aggregation_highway=True, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_attentive_match=False, wo_char=False, wo_full_match=False, wo_left_match=False, wo_max_attentive_match=False, wo_maxpool_match=False, wo_right_match=False, word_level_MP_dim=-1\n"
     ]
    }
   ],
   "source": [
    "config1=json.load(open('config1.json'))\n",
    "config2=open('config2.json').readline()\n",
    "print(config1)\n",
    "print(config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout_rate': '0.1', 'word_level_MP_dim': '-1', 'wo_maxpool_match': 'False', 'base_dir': \"'/u/zhigwang/zhigwang1/sentence_match/snli'\", 'wo_left_match': 'False', 'with_POS': 'False', 'wo_attentive_match': 'False', 'optimize_type': \"'adam'\", 'context_layer_num': '2', 'with_highway': 'True', 'with_lex_decomposition': 'False', 'POS_dim': '20', 'aggregation_lstm_dim': '100', 'wo_right_match': 'False', 'batch_size': '60', 'max_char_per_word': '10', 'learning_rate': '0.001', 'lex_decompsition_dim': '-1', 'suffix': \"'snli_7'\", 'with_filter_layer': 'False', 'wo_full_match': 'False', 'lambda_l2': '0.0', 'with_aggregation_highway': 'True', 'wo_char': 'False', 'context_lstm_dim': '100', 'with_NER': 'False', 'with_match_highway': 'True', 'NER_dim': '20', 'fix_word_vec': 'True', 'max_sent_length': '100', 'highway_layer_num': '1', 'wo_max_attentive_match': 'False', 'aggregation_layer_num': '2', 'MP_dim': '10', 'max_epochs': '10', 'char_emb_dim': '20', 'char_lstm_dim': '100'}\n"
     ]
    }
   ],
   "source": [
    "config2_str=config2\n",
    "config2={}\n",
    "cfs=config2_str.split(',')\n",
    "\n",
    "for cf in cfs:\n",
    "    name,val=cf.strip().split('=')\n",
    "    config2[name]=val\n",
    "print(config2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dir '/u/zhigwang/zhigwang1/sentence_match/snli'\n",
      "different: optimize_type config1: adam config2: 'adam'\n",
      "different: aggregation_lstm_dim config1: 300 config2: 100\n",
      "different: suffix config1: snli_newconfig_3class_rerun config2: 'snli_7'\n"
     ]
    }
   ],
   "source": [
    "for name in config2:\n",
    "    if name not in config1:\n",
    "        print(name,config2[name])\n",
    "    else:\n",
    "        if str(config1[name])!=str(config2[name]):\n",
    "            print('different:',name, 'config1:',config1[name],'config2:',config2[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63511486  0.2872599   0.2495    ]\n",
      " [ 0.58446731  0.29140842  0.18010312]] [[ 0.90722052  0.32068919  0.93383881]\n",
      " [ 0.42959248  0.15023892  0.78009962]] [[ 0.4294227   0.49792565  0.539976  ]\n",
      " [ 0.91242794  0.78873997  0.46472936]]\n",
      "[array([[ 1.97175813,  1.10587478,  1.72331476],\n",
      "       [ 1.92648768,  1.23038733,  1.42493212]], dtype=float32)]\n",
      "[[ 1.97175808  1.10587474  1.72331481]\n",
      " [ 1.92648773  1.23038731  1.4249321 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "tf_split=tf.placeholder(tf.float32,[None,None,None])\n",
    "# tf_index=tf.placeholder(tf.int32, [None,None])\n",
    "lengths=tf.placeholder(tf.int32,[None])\n",
    "lengths2=tf.placeholder(tf.int32,[None])\n",
    "mul2=tf.placeholder(tf.float32,[None])\n",
    "mul22=tf.placeholder(tf.float32,[None,None])\n",
    "totallength=lengths+lengths2\n",
    "n_input=tf.shape(lengths)[0]\n",
    "batch_idx=tf.range(n_input)\n",
    "gat_idx=tf.stack([batch_idx,lengths-1],axis=1)\n",
    "# part_res=tf.gather_nd(tf_split,tf_index)\n",
    "part_res=tf.gather_nd(tf_split, gat_idx)\n",
    "mul_input=tf.placeholder(tf.float32,[None,None])\n",
    "mul_part_res=tf.multiply(mul_input,mul2)\n",
    "tf_c=tf_split[:,-1,:]\n",
    "\n",
    "tfadd1=tf.placeholder(tf.float32,[None,None])\n",
    "tfadd2=tf.placeholder(tf.float32,[None,None])\n",
    "tfadd3=tf.placeholder(tf.float32,[None,None])\n",
    "tf_res=tf.add_n([tfadd1,tfadd2,tfadd3])\n",
    "\n",
    "npadd1=np.random.rand(2,3)\n",
    "npadd2=np.random.rand(2,3)\n",
    "npadd3=np.random.rand(2,3)\n",
    "input_dict={tfadd1:npadd1,tfadd2:npadd2,tfadd3:npadd3}\n",
    "\n",
    "# a=np.random.rand(2,3,2)\n",
    "# # b=np.array([[[0,0],[0,2]],[[1,0],[1,2]]])\n",
    "# # b=np.array([[0,1],[1,2]])\n",
    "# # input_dict={tf_split:a,tf_index:b}\n",
    "# c=np.array([1,3])\n",
    "# d=np.array([3,4,5])\n",
    "# dp=np.array([[3],[4]])\n",
    "# ee=np.random.rand(2,3)\n",
    "# print(dp.shape)\n",
    "# # input_dict={tf_split:a, lengths:c,lengths2:d}\n",
    "# input_dict={mul_input:ee,mul2:d}\n",
    "# print(input_dict)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    res=sess.run([tf_res],feed_dict=input_dict)\n",
    "    print(npadd1,npadd2,npadd3)\n",
    "    print(res)\n",
    "    print(npadd1+npadd2+npadd3)\n",
    "#     res_c,res,res_tot=sess.run([tf_c, part_res,totallength],feed_dict=input_dict)\n",
    "\n",
    "#     print(a)\n",
    "#     print(res)\n",
    "#     print(res_c)\n",
    "#     print(res_tot)\n",
    "#     print(res_mul)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dfs-5'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dfs-{}'.format(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "a=json.loads('[1,2,3]')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333531750779\n",
      "[ 0.0833821   0.08337574  0.0833765   0.08339741]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=[-2.48432159, -2.48439789, -2.48438883, -2.48413801]\n",
    "b=[0.33353174,  0.33281344,  0.33365488]\n",
    "print(sum(np.exp(a)))\n",
    "print(np.exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "a=pickle.load(open(r'D:\\users\\t-yicxu\\BiMPM_1.0\\model_data\\res.pkg','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "return_list=a\n",
    "_,loss_value, pred, prob, all_probs, correct, gate_prob, gate_log_prob, weighted_log_probs,\\\n",
    "                    log_coeffs=return_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33365992,  0.33400437,  0.33233574],\n",
       "       [ 0.3339709 ,  0.33372563,  0.33230346]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gate_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33389902,  0.33328748,  0.33281353],\n",
       "       [ 0.3337996 ,  0.33331674,  0.33288369]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(gate_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_probs=np.exp(weighted_log_probs)\n",
    "weighted_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24998248,  0.2500056 ,  0.24998039,  0.25003159],\n",
       "       [ 0.24999011,  0.24996927,  0.2500304 ,  0.25001025]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24998248,  0.2500056 ,  0.24998039,  0.25003159],\n",
       "       [ 0.24999011,  0.24996924,  0.25003043,  0.25001025]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(weighted_probs,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.24993956,  0.25000218,  0.25000912,  0.25004914],\n",
       "        [ 0.25003448,  0.25002894,  0.25000015,  0.24993648]],\n",
       "\n",
       "       [[ 0.25000593,  0.25002992,  0.25002801,  0.24993609],\n",
       "        [ 0.25001737,  0.24999259,  0.24999046,  0.24999969]],\n",
       "\n",
       "       [[ 0.24998565,  0.24997634,  0.25001654,  0.25002146],\n",
       "        [ 0.24996518,  0.25003907,  0.25004691,  0.2499488 ]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp=np.tile(gate_prob.transpose(),(1,1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.33365992],\n",
       "        [ 0.3339709 ]],\n",
       "\n",
       "       [[ 0.33400437],\n",
       "        [ 0.33372563]],\n",
       "\n",
       "       [[ 0.33233574],\n",
       "        [ 0.33230346]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp=np.expand_dims(gate_prob.transpose(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp=np.tile(tmp,(1,1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.33365992,  0.33365992,  0.33365992,  0.33365992],\n",
       "        [ 0.3339709 ,  0.3339709 ,  0.3339709 ,  0.3339709 ]],\n",
       "\n",
       "       [[ 0.33400437,  0.33400437,  0.33400437,  0.33400437],\n",
       "        [ 0.33372563,  0.33372563,  0.33372563,  0.33372563]],\n",
       "\n",
       "       [[ 0.33233574,  0.33233574,  0.33233574,  0.33233574],\n",
       "        [ 0.33230346,  0.33230346,  0.33230346,  0.33230346]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24997705,  0.25000286,  0.25001791,  0.25000221],\n",
       "       [ 0.25000575,  0.25002018,  0.25001246,  0.24996166]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tmp*all_probs,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24997705,  0.25000286,  0.25001791,  0.25000221],\n",
       "       [ 0.25000575,  0.25002018,  0.25001246,  0.24996166]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24997705,  0.25000286,  0.25001788,  0.25000218],\n",
       "       [ 0.25000575,  0.25002018,  0.25001246,  0.24996167]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_probs=np.exp(weighted_log_probs)\n",
    "weighted_probs.shape\n",
    "np.sum(weighted_probs,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4328276729559748\n"
     ]
    }
   ],
   "source": [
    "num_all=19736\n",
    "num_middle=5744\n",
    "num_high=num_all-num_middle\n",
    "\n",
    "correct_all=45.36/100\n",
    "correct_middle=50.42/100\n",
    "\n",
    "correct_high=(num_all*correct_all-correct_middle*num_middle)/num_high\n",
    "print(correct_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4371286449399657\n"
     ]
    }
   ],
   "source": [
    "num_all=19736\n",
    "num_middle=5744\n",
    "num_high=num_all-num_middle\n",
    "\n",
    "correct_all=45.24/100\n",
    "correct_middle=48.96/100\n",
    "\n",
    "correct_high=(num_all*correct_all-correct_middle*num_middle)/num_high\n",
    "print(correct_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
