{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "print('???')\n",
    "mode='train'\n",
    "n_ans='same'\n",
    "n_sent=0\n",
    "span_mode='f1'  # exact or overlap or f1\n",
    "num_class=2\n",
    "predict=False\n",
    "verbose=False\n",
    "# input_data=open(r'D:\\users\\t-yicxu\\data\\squad\\\\'+mode+'-v1.1.json',encoding='utf-8')\n",
    "input_data=open(r'D:\\users\\t-yicxu\\data\\squad\\\\'+mode+'\\\\'+mode+'-stanford.json',encoding='utf-8')\n",
    "if mode=='dev':\n",
    "\tdump_data=open(r'D:\\users\\t-yicxu\\biglearn\\res_v16_dev.score.0.dump',encoding='utf-8')\n",
    "else:\n",
    "\tdump_data=open(r'D:\\users\\t-yicxu\\biglearn\\res_v16_train.score.0.dump',encoding='utf-8')\n",
    "output_file=open(r'D:\\users\\t-yicxu\\data\\squad\\entail_'+mode+'_%s_%d_%s_%dclass_tmp.tsv' %(str(n_ans),n_sent,span_mode,num_class),'w',encoding='utf-8')\n",
    "\n",
    "print('begin')\n",
    "\n",
    "texts=[]\n",
    "hyps=[]\n",
    "labels=[]\n",
    "ids=[]\n",
    "prediction={}\n",
    "\n",
    "# all_data=json.load(input_data)\n",
    "all_data={'data':[]}\n",
    "line=input_data.readline()\n",
    "# print(line)\n",
    "assert line.strip()=='SQuDA'\n",
    "for line in input_data:\n",
    "\tall_data['data'].append(json.loads(line))\n",
    "print('finish reading lines')\n",
    "\n",
    "proc_all_data=[]\n",
    "for (ii,data) in enumerate(all_data['data']):\n",
    "\tif ii % 1000==0:\n",
    "\t\tprint('proc data',ii)\n",
    "\ttry:\n",
    "\t\t# assert len(data['answer_pos'])==1\n",
    "\t\tassert len(data['answer_pos'][0])==2\n",
    "\t\tassert len(data['answer_pos'][0][0])==2\n",
    "\texcept AssertionError:\n",
    "\t\tprint(data['answer_pos'])\n",
    "\t\tinput('check')\n",
    "\tanswer_datas=data['answer_pos']\n",
    "\tans_data_collection=set()\n",
    "\tfor a_data in answer_datas:\n",
    "\t\tans_data_collection.add(((a_data[0][0],a_data[0][1]),(a_data[1][0],a_data[1][1])))\n",
    "\tnew_answers=[]\n",
    "\tfor tup in ans_data_collection:\n",
    "\t\tnew_answers.append([[tup[0][0],tup[0][1]],[tup[1][0],tup[1][1]]])\n",
    "\tif verbose:\n",
    "\t\tprint('new_answers=',new_answers)\n",
    "\tdump_line=next(dump_data)\n",
    "\tfor ans_span in new_answers:\n",
    "\n",
    "\t\tgt_sent=range(ans_span[0][0],ans_span[1][0]+1)\n",
    "\t\tgt_sent_texts=[' '.join([t for t in data['context_tokens'][k]]) for k in gt_sent]\n",
    "\t\tgt_text=' '.join(gt_sent_texts)\n",
    "\t\tques_text=' '.join(data['question_tokens'])\n",
    "\t\tn_context_token=sum([len(sent) for sent in data['context_tokens']])\n",
    "\n",
    "\n",
    "\t\t# Insert ground truth\n",
    "\t\tgt_ans_words=[]\n",
    "\t\tfor sentid in gt_sent:\n",
    "\t\t\tst_pos=ans_span[0][1] if sentid==ans_span[0][0] else 0\n",
    "\t\t\tend_pos=ans_span[1][1] if sentid==ans_span[1][0] else len(data['context_tokens'][sentid])-1\n",
    "\t\t\t# print(st_pos,end_pos)\n",
    "\t\t\tfor idx in range(st_pos,end_pos+1):\n",
    "\t\t\t\tgt_ans_words.append(data['context_tokens'][sentid][idx])\n",
    "\t\tgt_ans=' '.join(gt_ans_words)\n",
    "\t\tlabels.append(1)\n",
    "\t\ttexts.append(gt_text)\n",
    "\t\tthishyp=' '.join([ques_text,gt_ans])\n",
    "\t\thyps.append(thishyp)\n",
    "\t\tids.append(data['id']+'_gt')\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('label=%d, text=%s, hyp=%s' % (labels[-1],texts[-1],hyps[-1]))\n",
    "\t\t\tinput('check')\n",
    "\n",
    "\t\t#insert wrong texts\n",
    "\t\tavailable_sents=set(range(len(data['context_tokens'])))-set(gt_sent)\n",
    "\t\tchoose_num=min(n_sent,len(available_sents))\n",
    "\t\tif choose_num==0:\n",
    "\t\t\tcontinue\n",
    "\t\tchosen_sents=np.random.choice(list(available_sents),choose_num)\n",
    "\t\tfor sentid in chosen_sents:\n",
    "\t\t\ttexts.append(' '.join(data['context_tokens'][sentid]))\n",
    "\t\t\thyps.append(' '.join([ques_text,gt_ans]))\n",
    "\t\t\tif num_class==2:\n",
    "\t\t\t\tlabels.append(0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabels.append(3)\n",
    "\t\t\tids.append(data['id']+'_sent')\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint('label=%d, text=%s, hyp=%s' % (labels[-1],texts[-1],hyps[-1]))\n",
    "\t\t\t\tinput('check')\t\t\t\n",
    "\n",
    "\t#insert potential wrong answers\n",
    "\tspan_probs=[]\n",
    "\tspans=[]\n",
    "\t\n",
    "\ttoken_probs=dump_line.split(' ')\n",
    "\n",
    "\t# if i>=len(proc_passages):\n",
    "\t# \tbreak\n",
    "\tif len(token_probs)!=n_context_token:\n",
    "\t\tprint('question',ii)\n",
    "\t\tprint('prediction length=', len(token_probs))\n",
    "\t\tprint('tokenize length=',n_context_token)\n",
    "\t\tprint(data)\n",
    "\t\tprint(dump_line)\n",
    "\t\tinput('check')\n",
    "\t\tcontinue\n",
    "\tstartps=[]\n",
    "\tendps=[]\n",
    "\tfor (tid,tp) in enumerate(token_probs):\n",
    "\t\tpid,startp,endp=tp.split('#')\n",
    "\t\tassert tid==int(pid)\n",
    "\t\tstartps.append(float(startp))\n",
    "\t\tendps.append(float(endp))\n",
    "\t# print(startps)\n",
    "\t# print(endps)\n",
    "\t# input('check')\n",
    "\tfor id1 in range(n_context_token):\n",
    "\t\tfor id2 in range(id1,min(n_context_token,id1+15)):\n",
    "\t\t\tspan_probs.append(-startps[id1]*endps[id2])\n",
    "\t\t\tspans.append((id1,id2))\n",
    "\tspan_rank=np.argsort(span_probs)\n",
    "\tpartsum=[0]\n",
    "\tall_tokens=sum(data['context_tokens'],[])\n",
    "\tfor sid in range(len(data['context_tokens'])):\n",
    "\t\tpartsum.append(partsum[-1]+len(data['context_tokens'][sid]))\n",
    "\n",
    "\tadded_count=0\n",
    "\tif n_ans=='same':\n",
    "\t\ttarget_num=len(new_answers)\n",
    "\telse:\n",
    "\t\ttarget_num=n_ans\n",
    "\tfirst_ten_perm=np.random.permutation(10)\n",
    "\tfor tmpi in range(len(spans)):\n",
    "\t\tif tmpi<10:\n",
    "\t\t\ti=first_ten_perm[tmpi]\n",
    "\t\telse:\n",
    "\t\t\ti=tmpi\n",
    "\t\tthis_start=spans[span_rank[i]][0]\n",
    "\t\tthis_end=spans[span_rank[i]][1]\n",
    "\n",
    "\t\tthis_ans=' '.join([all_tokens[idx] for idx in range(this_start,this_end+1)])\n",
    "\t\tif i==0:\n",
    "\t\t\tprediction[data['id']]=this_ans\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('this_start=',this_start,'this_end=',this_end)\n",
    "\t\t\t# print('gt_start=',gt_span_start,'gt_end=',gt_span_end)\n",
    "\n",
    "\t\tcanuse=True\n",
    "\t\tfor a_gt_span in new_answers:\n",
    "\t\t\ta_gt_span_start=partsum[a_gt_span[0][0]]+a_gt_span[0][1]\n",
    "\t\t\ta_gt_span_end=partsum[a_gt_span[1][0]]+a_gt_span[1][1]\n",
    "\t\t\tif span_mode=='exact':\n",
    "\t\t\t\tif this_start==a_gt_span_start and this_end==a_gt_span_end:\n",
    "\t\t\t\t\tcanuse=False\n",
    "\t\t\telif span_mode=='f1':\n",
    "\t\t\t\toverlap_length=max(min(a_gt_span_end,this_end)+1-max(a_gt_span_start,this_start),0)\n",
    "\t\t\t\tif overlap_length==0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprecision=overlap_length/(this_end-this_start+1)\n",
    "\t\t\t\t\trecall=overlap_length/(a_gt_span_end - a_gt_span_start+1)\n",
    "\t\t\t\t\tf1=2*precision*recall/(precision+recall)\n",
    "\t\t\t\t\tif f1>=0.5:\n",
    "\t\t\t\t\t\tcanuse=False\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tassert span_mode=='overlap'\n",
    "\t\t\t\tif (this_start - a_gt_span_end)*(this_end - a_gt_span_start)<=0:\n",
    "\t\t\t\t\tcanuse=False\n",
    "\n",
    "\n",
    "\t\tif not canuse:\n",
    "\t\t\tcontinue\n",
    "\t\tfor start_sent in range(len(partsum)):\n",
    "\t\t\tif partsum[start_sent+1]>=this_start:\n",
    "\t\t\t\tbreak\n",
    "\t\tfor end_sent in range(len(partsum)):\n",
    "\t\t\tif partsum[end_sent]>=this_end:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\ttext_sent=range(start_sent,end_sent)\n",
    "\t\tthis_text=' '.join([' '.join([t for t in data['context_tokens'][k]]) for k in text_sent])\n",
    "\n",
    "\t\ttexts.append(this_text)\n",
    "\t\tthis_ans=' '.join([all_tokens[idx] for idx in range(this_start,this_end+1)])\n",
    "\t\tthishyp=' '.join([ques_text,this_ans])\n",
    "\t\thyps.append(thishyp)\n",
    "\t\tif num_class==2:\n",
    "\t\t\tlabels.append(0)\n",
    "\t\telse:\n",
    "\t\t\tlabels.append(2)\n",
    "\t\tids.append(data['id']+'_ans_%d' % (tmpi))\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('label=%d, text=%s, hyp=%s, id=%s' % (labels[-1],texts[-1],hyps[-1],ids[-1]))\n",
    "\t\t\tinput('check')\n",
    "\t\tadded_count+=1\n",
    "\t\tif added_count==target_num:\n",
    "\t\t\tbreak\n",
    "\n",
    "\n",
    "\n",
    "\t\t# print('inside here')\n",
    "\t\t# break\n",
    "if predict:\n",
    "\tpredict_out=open(r'D:\\users\\t-yicxu\\BiMPM_1.0\\model_data\\sample_predict_tmp.json','w',encoding='utf-8')\n",
    "\tjson.dump(prediction,predict_out,ensure_ascii=False)\n",
    "for i in range(len(hyps)):\n",
    "\tprint('%d\\t%s\\t%s\\t%s' % (labels[i], texts[i],hyps[i],ids[i]),file=output_file)\t\n",
    "text_lens=[len(a) for a in texts]\n",
    "hyp_lens=[len(a) for a in hyps]\n",
    "\n",
    "n_words_text=[]\n",
    "n_words_hyp=[]\n",
    "for i in range(len(hyps)):\n",
    "\tn_words=len(hyps[i].split(' '))\n",
    "\tn_words_hyp.append(n_words)\n",
    "\n",
    "\tn_words=len(texts[i].split(' '))\n",
    "\tn_words_text.append(n_words)\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(text_lens))\n",
    "print('mean hyp length:',np.mean(hyp_lens))\n",
    "\n",
    "print('max text length:',np.max(text_lens))\n",
    "print('max hyp length:',np.max(hyp_lens))\n",
    "\n",
    "print('min text length:',np.min(text_lens))\n",
    "print('min hyp length:',np.min(hyp_lens))\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(n_words_text))\n",
    "print('mean hyp length:',np.mean(n_words_hyp))\n",
    "\n",
    "print('max text length:',np.max(n_words_text))\n",
    "print('max hyp length:',np.max(n_words_hyp))\n",
    "print('min text length:',np.min(n_words_text))\n",
    "print('min hyp length:',np.min(n_words_hyp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_words_text=[]\n",
    "n_words_hyp=[]\n",
    "for i in range(len(hyps)):\n",
    "\tn_words=len(hyps[i].split(' '))\n",
    "\tn_words_hyp.append(n_words)\n",
    "\n",
    "\tn_words=len(texts[i].split(' '))\n",
    "\tn_words_text.append(n_words)\n",
    "\n",
    "\n",
    "print('mean text length:',np.mean(text_lens))\n",
    "print('mean hyp length:',np.mean(hyp_lens))\n",
    "\n",
    "print('max text length:',np.max(text_lens))\n",
    "print('max hyp length:',np.max(hyp_lens))\n",
    "\n",
    "print('mean text length:',np.mean(n_words_text))\n",
    "print('mean hyp length:',np.mean(n_words_hyp))\n",
    "\n",
    "print('max text length:',np.max(n_words_text))\n",
    "print('max hyp length:',np.max(n_words_hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
